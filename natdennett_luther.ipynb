{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from time import time, sleep\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Song data structure\n",
    "# Remember: namedtuples require that you plug in \n",
    "# every data type.\n",
    "\n",
    "Song = namedtuple('Song', ['header', 'verified', 'metadata', 'lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the chromedriver executable\n",
    "def start_driver(url):\n",
    "\n",
    "    \"\"\"This function will open a Chrome browser at the given url.\n",
    "    Input: a valid URL as a string.\n",
    "    Doesn't return anything, but creates a global variable *driver*\n",
    "    that can be called upon elsewhere.\"\"\"\n",
    "    \n",
    "    chromedriver = \"/Applications/chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "    \n",
    "    # Using the global keyword so that we can call upon the driver\n",
    "    # further down in the code.\n",
    "    \n",
    "    global driver \n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by opening the all rap songs page.\n",
    "\n",
    "all_rap_songs_genius = \"https://genius.com/tags/rap/all\"\n",
    "start_driver(all_rap_songs_genius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to scroll down to the bottom of the screen\n",
    "# This only works if you manually focus on the Chrome browser.\n",
    "\n",
    "for i in range (10):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet is there to grab links to songs\n",
    "# on the https://genius.com/tags/rap/all page.\n",
    "# It appears that 1000 is the most search results\n",
    "# that genius.com will display at one time.\n",
    "\n",
    "\n",
    "link_selector = '//a[@class= \" song_link\"]'\n",
    "top_songs = driver.find_elements_by_xpath(link_selector)\n",
    "\n",
    "\n",
    "link_list = []\n",
    "\n",
    "for elem in top_songs:\n",
    "    link_list.append(elem.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I save the list of links as a pickle.\n",
    "# That way, I don't have to do this again.\n",
    "\n",
    "with open('links.pkl', 'wb') as link_pickle:\n",
    "    pickle.dump(link_list,link_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I pick up again I can just pull from the pickle\n",
    "\n",
    "with open('links.pkl', 'rb') as link_pickle:\n",
    "    link_list = pickle.load(link_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(list_of_urls):\n",
    "    \n",
    "    \"\"\"This function uses the Chrome driver to visit \n",
    "    every page on a list of Genius links,\n",
    "    and gathers the info I want to analyze.\n",
    "    Input needs to be:\n",
    "    a list of links to Genius pages, otherwise it won't work. \n",
    "    Output will be:\n",
    "    a dict of named tuples of the Song format.\"\"\"\n",
    "\n",
    "    dict_of_songs = defaultdict(Song)\n",
    "\n",
    "    for i, url in enumerate(list_of_urls):\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(.5+2*random.random())\n",
    "\n",
    "        # Gather and filter metadata\n",
    "\n",
    "        views_regex = re.compile('views', re.IGNORECASE)\n",
    "        contrib_regex = re.compile('contributors', re.IGNORECASE)\n",
    "        tag_regex = re.compile('(\\w+,\\s\\w+)+')\n",
    "\n",
    "        all_metadata = driver.find_elements_by_class_name('metadata_with_icon')\n",
    "\n",
    "        filtered_metadata = [metadata.text for metadata in all_metadata\n",
    "                             if re.search(views_regex, metadata.text)\n",
    "                             or re.search(contrib_regex, metadata.text)\n",
    "                             or re.search(tag_regex, metadata.text)]\n",
    "\n",
    "        # Scrape raw lyrics\n",
    "\n",
    "        lyrics_string = ''\n",
    "\n",
    "        lyrics = driver.find_elements_by_class_name('song_body-lyrics')\n",
    "        for lyric in lyrics:\n",
    "            lyrics_string += lyric.text\n",
    "\n",
    "        # Scrape header\n",
    "\n",
    "        header_string = driver.find_element_by_class_name(\n",
    "            'header_with_cover_art-primary_info').text\n",
    "\n",
    "        # Determine whether artist has contributed to their song page\n",
    "\n",
    "        try:\n",
    "            ver_art = driver.find_element_by_class_name(\n",
    "                'song_verified_artists-section')\n",
    "            verified_bool = True\n",
    "        except:\n",
    "            verified_bool = False\n",
    "            \n",
    "        # Create Song named tuple and add to dictionary\n",
    "\n",
    "        dict_of_songs[url] = Song(header=header_string, lyrics=lyrics_string,\n",
    "                                  verified=verified_bool, metadata=filtered_metadata)\n",
    "        if (i+1 % 100 == 0):\n",
    "            time.sleep(320)\n",
    "\n",
    "    return dict_of_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling the song dict\n",
    "with open('songs_dict.pkl', 'wb') as song_file:\n",
    "    pickle.dump(song_dict, song_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickling the song dict\n",
    "with open('songs_dict.pkl', 'rb') as song_file:\n",
    "    song_dict = pickle.load(song_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(raw_lyrics):\n",
    "    \n",
    "    # This code snippet removes the header\n",
    "\n",
    "    header_regex = re.compile('^.+\\n')\n",
    "        \n",
    "    # This code removes the footer\n",
    "\n",
    "    footer_regex = re.compile('\\d+ Embed.+', re.DOTALL)\n",
    "    \n",
    "    # This should remove the little snippets in brackets\n",
    "\n",
    "    bracket_regex = re.compile(r'\\[.+\\]')\n",
    "    \n",
    "    # This should remove the special characters that creep in\n",
    "    \n",
    "    character_regex = re.compile(r'''[\\(\\)\\'\\\"\\!\\?]''')\n",
    "    \n",
    "    cleaned_lyrics = re.sub(footer_regex, '', raw_lyrics)\n",
    "    cleaned_lyrics = re.sub(header_regex, '', cleaned_lyrics)\n",
    "    cleaned_lyrics = re.sub(bracket_regex, '', cleaned_lyrics)\n",
    "    cleaned_lyrics = re.sub(character_regex, '', cleaned_lyrics)\n",
    "    \n",
    "    return cleaned_lyrics.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_regex(list_of_words):\n",
    "    \n",
    "    full_string = list_of_words[0]\n",
    "    for word in list_of_words[1:]:\n",
    "        new_word = '|' + word\n",
    "        full_string += new_word\n",
    "    return re.compile(full_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_semantic_fields(path):\n",
    "    # semantic fields\n",
    "    fields = []\n",
    "    with open(path) as csv_file:\n",
    "        dict_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in dict_reader:\n",
    "            fields.append(row)\n",
    "    \n",
    "    global religion_regex, curse_regex, clothes_regex, cars_regex, foreign_sem_field\n",
    "\n",
    "    religion_regex = list_to_regex(fields[0])\n",
    "    curse_regex = list_to_regex(fields[1])\n",
    "    clothes_regex = list_to_regex(fields[2])\n",
    "    cars_regex = list_to_regex(fields[3])\n",
    "    \n",
    "    # names of tags I want to filter out\n",
    "    foreign_sem_field = set(fields[4])\n",
    "\n",
    "load_semantic_fields('semantic_fields.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_songs(dict_of_songs):\n",
    "    \n",
    "    \"\"\"This function takes a dictionary of Song named tuples\n",
    "    and returns a list of dictionaries with just the numerical and categorical info\n",
    "    for each song which to be used in the dataframe.\"\"\"\n",
    "\n",
    "    dict_list = []\n",
    "\n",
    "    for link in dict_of_songs:\n",
    "        song = dict_of_songs[link]\n",
    "\n",
    "        info_dict = {'title': '',\n",
    "                     'artist': '',\n",
    "                     'words': 0,\n",
    "                     'views': 0,\n",
    "                     'contrib': 0,\n",
    "                     'tags': '',\n",
    "                     'religion': 0,\n",
    "                     'curse': 0,\n",
    "                     'clothes': 0,\n",
    "                     'cars': 0,\n",
    "                     'producer': '',\n",
    "                     'verified': None}\n",
    "\n",
    "        if song.verified:\n",
    "            info_dict['verified'] = 1\n",
    "        else:\n",
    "            info_dict['verified'] = 0\n",
    "        \n",
    "        lyrics = clean_lyrics(song.lyrics)\n",
    "        word_list = lyrics.split()\n",
    "        song_length = len(word_list)\n",
    "        info_dict['words'] = song_length\n",
    "        \n",
    "        info_dict['religion'] = len(re.findall(religion_regex,lyrics)) / song_length\n",
    "        info_dict['curse'] = len(re.findall(curse_regex,lyrics)) / song_length\n",
    "        info_dict['clothes'] = len(re.findall(clothes_regex,lyrics)) / song_length\n",
    "        info_dict['cars'] = len(re.findall(cars_regex,lyrics)) / song_length\n",
    "\n",
    "        # This should extract the number of views from the metadata\n",
    "\n",
    "        info_dict['views'] = int(song.metadata[0].split()[0].replace(',', ''))\n",
    "        \n",
    "        # This should extract the number of contributors from the metadata\n",
    "\n",
    "        info_dict['contrib'] = int(song.metadata[1].split()[0].replace(',', ''))\n",
    "        \n",
    "        # This should get the tags\n",
    "        \n",
    "        try:\n",
    "            info_dict['tags'] = song.metadata[2]\n",
    "        except:\n",
    "            info_dict['tags'] = ''\n",
    "\n",
    "        # This should get the title from the header\n",
    "\n",
    "        info_dict['title'] = song.header.split('\\n')[0]\n",
    "\n",
    "        # This should get the artist name from the header\n",
    "\n",
    "        info_dict['artist'] = song.header.split('\\n')[1]\n",
    "\n",
    "        # This should get the producer name from the header\n",
    "        # If there is no producer, this should be None.\n",
    "\n",
    "        producer_regex = re.compile('Produced by (.+)')\n",
    "        try:\n",
    "            info_dict['producer'] = re.match(producer_regex, song.header.split('\\n')[2]).group(1)\n",
    "        except:\n",
    "            info_dict['producer'] = ''\n",
    "        \n",
    "        dict_list.append(info_dict)\n",
    "\n",
    "        \n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreign_to_nan(tags):\n",
    "    \n",
    "    \"\"\"I use this to filter out songs that are in a foreign language.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not tags:\n",
    "        return tags\n",
    "    for tag in tags.split(', '):\n",
    "        if tag in foreign_sem_field:\n",
    "            return np.nan\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once you've pulled the song_dict to initialize the dataframe.\n",
    "\n",
    "dl = process_songs(song_dict)\n",
    "\n",
    "df = pd.DataFrame(dl)\n",
    "\n",
    "df['tags'] = df.tags.apply(lambda x: foreign_to_nan(x))\n",
    "\n",
    "df = df.dropna(axis = 0, how = 'any')\n",
    "\n",
    "df['logviews'] = np.log2(df['views'])\n",
    "\n",
    "\n",
    "# Removing the very top changes the results significantly\n",
    "\n",
    "mask = df['views'] > 5000000\n",
    "\n",
    "df = df.drop(df[mask].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23492132538947075"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['cars', 'clothes', 'curse', 'religion', 'verified','contrib']], df['logviews'], test_size=0.33, random_state=20)\n",
    "m = LinearRegression()\n",
    "m.fit(X_train, y_train)\n",
    "m.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.48568636e+00,  6.11674316e+00, -1.63021719e-01, -6.22221093e+00,\n",
       "        1.27696675e-01,  3.31050054e-03])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1918983065680412"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2832240049412208"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['cars', 'clothes', 'curse', 'religion', 'verified','contrib']], df['views'], test_size=0.33, random_state=22)\n",
    "p = PolynomialFeatures()\n",
    "Xpoly = p.fit_transform(X_train)\n",
    "Xpolytest = p.transform(X_test)\n",
    "m = LinearRegression()\n",
    "m.fit(Xpoly,y_train)\n",
    "m.score(Xpoly,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00451711483799766"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(Xpolytest,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 28 songs above 5 million views\n",
    "\n",
    "len(old_df[mask])\n",
    "\n",
    "# 26 foreign songs\n",
    "\n",
    "len(old_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_df)\n",
    "old_df['tags'] = old_df.tags.apply(lambda x: foreign_to_nan(x))\n",
    "\n",
    "old_df = old_df.dropna(axis = 0, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_validate(X, y):\n",
    "    \n",
    "    '''\n",
    "    For a set of features and target X, y, perform a 80/20 train/val split, \n",
    "    fit and validate a linear regression model, and report results\n",
    "    '''\n",
    "    \n",
    "    # perform train/val split\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # fit linear regression to training data\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # score fit model on validation data\n",
    "    val_score = lr_model.score(X_val, y_val)\n",
    "    \n",
    "    # report results\n",
    "    print('\\nValidation R^2 score was:', val_score)\n",
    "    print('Feature coefficient results: \\n')\n",
    "    for feature, coef in zip(X.columns, lr_model.coef_):\n",
    "        print(feature, ':', coef) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation R^2 score was: 0.18248245197482094\n",
      "Feature coefficient results: \n",
      "\n",
      "cars : [ 6.64591842e+00 -3.04214649e+00  9.13226688e-02 -1.02253066e+00\n",
      "  3.57152584e-02  1.12507275e-03]\n"
     ]
    }
   ],
   "source": [
    "split_and_validate(df.loc[:,['cars', 'clothes', 'curse', 'religion', 'verified','contrib']],df.loc[:,['logviews']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = df[df['cars'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "curse_df = df[df['curse'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>logviews</td>     <th>  R-squared:         </th> <td>   0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   45.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Apr 2019</td> <th>  Prob (F-statistic):</th> <td>5.29e-49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:45:41</td>     <th>  Log-Likelihood:    </th> <td> -868.82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   946</td>      <th>  AIC:               </th> <td>   1752.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   939</td>      <th>  BIC:               </th> <td>   1786.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   19.6508</td> <td>    0.048</td> <td>  410.727</td> <td> 0.000</td> <td>   19.557</td> <td>   19.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cars</th>      <td>    6.8875</td> <td>   11.816</td> <td>    0.583</td> <td> 0.560</td> <td>  -16.301</td> <td>   30.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clothes</th>   <td>    4.1706</td> <td>    2.929</td> <td>    1.424</td> <td> 0.155</td> <td>   -1.577</td> <td>    9.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>curse</th>     <td>   -0.1967</td> <td>    0.918</td> <td>   -0.214</td> <td> 0.830</td> <td>   -1.998</td> <td>    1.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>religion</th>  <td>   -1.2835</td> <td>    3.109</td> <td>   -0.413</td> <td> 0.680</td> <td>   -7.385</td> <td>    4.818</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>verified</th>  <td>    0.1217</td> <td>    0.042</td> <td>    2.910</td> <td> 0.004</td> <td>    0.040</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contrib</th>   <td>    0.0032</td> <td>    0.000</td> <td>   15.533</td> <td> 0.000</td> <td>    0.003</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>49.518</td> <th>  Durbin-Watson:     </th> <td>   0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  54.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.570</td> <th>  Prob(JB):          </th> <td>1.43e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.711</td> <th>  Cond. No.          </th> <td>1.22e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.22e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               logviews   R-squared:                       0.225\n",
       "Model:                            OLS   Adj. R-squared:                  0.220\n",
       "Method:                 Least Squares   F-statistic:                     45.50\n",
       "Date:                Thu, 18 Apr 2019   Prob (F-statistic):           5.29e-49\n",
       "Time:                        14:45:41   Log-Likelihood:                -868.82\n",
       "No. Observations:                 946   AIC:                             1752.\n",
       "Df Residuals:                     939   BIC:                             1786.\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     19.6508      0.048    410.727      0.000      19.557      19.745\n",
       "cars           6.8875     11.816      0.583      0.560     -16.301      30.076\n",
       "clothes        4.1706      2.929      1.424      0.155      -1.577       9.918\n",
       "curse         -0.1967      0.918     -0.214      0.830      -1.998       1.604\n",
       "religion      -1.2835      3.109     -0.413      0.680      -7.385       4.818\n",
       "verified       0.1217      0.042      2.910      0.004       0.040       0.204\n",
       "contrib        0.0032      0.000     15.533      0.000       0.003       0.004\n",
       "==============================================================================\n",
       "Omnibus:                       49.518   Durbin-Watson:                   0.451\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               54.549\n",
       "Skew:                           0.570   Prob(JB):                     1.43e-12\n",
       "Kurtosis:                       2.711   Cond. No.                     1.22e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.22e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "lm1 = smf.ols('logviews ~ cars + clothes + curse + religion + verified + contrib', data=df)\n",
    "\n",
    "# Fit the model\n",
    "fit1 = lm1.fit()\n",
    "\n",
    "# Print summary statistics of the model's performance\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"I realized that there were some songs where\n",
    "the driver failed to scrape the metadata.\n",
    "Here's the code I used to fix it:\n",
    "songs_to_fix = []\n",
    "for key in song_dict:\n",
    "    if not song_dict[key].metadata:\n",
    "        songs_to_fix.append(key)\n",
    "        \n",
    "start_driver(songs_to_fix[0])\n",
    "missing_info = scrape_info(songs_to_fix)\n",
    "for key in missing_info:\n",
    "    song_dict[key] = missing_info[key]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_links = [mark.format(i) for i in range(1,16)]\n",
    "\n",
    "mark = 'https://www.biblegateway.com/passage/?search=Mark+{}&version=NIV'\n",
    "\n",
    "full_text = ''\n",
    "\n",
    "for link in bible_links:\n",
    "\n",
    "    response = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    NIV_tags = soup.find_all(class_='version-NIV')\n",
    "\n",
    "    text_list = [tag.text for tag in NIV_tags]\n",
    "\n",
    "    bible_text = re.sub('\\d+\\\\xa0','',text_list[0])\n",
    "    bible_text = re.sub('\\[.\\]','',bible_text)\n",
    "    bible_text = re.sub('\\\\xa0', '', bible_text)\n",
    "    footnote_regex = re.compile('Footnotes.+',re.DOTALL)\n",
    "    bible_text = re.sub(footnote_regex,'',bible_text)\n",
    "    \n",
    "    full_text += bible_text\n",
    "\n",
    "full_text = clean_lyrics(full_text)\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "\n",
    "for word in full_text.split():\n",
    "    freq_dict[word] += 1\n",
    "\n",
    "freq_dict\n",
    "\n",
    "text_analysis = pd.Series(freq_dict)\n",
    "\n",
    "freq_words = text_analysis[text_analysis > 5]\n",
    "\n",
    "\n",
    "freq_words = freq_words.sort_values()\n",
    "\n",
    "freq_words.drop(['the', 'and','to','of', 'he', 'they', 'a', 'in', 'you', 'his', 'with', 'him', 'is', 'will', 'for', 'that', 'not', 'was', 'it', 'on', 'but', 'be', 'were', 'when', 'them', 'i', 'had', 'at', 'one', 'out', 'said'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
